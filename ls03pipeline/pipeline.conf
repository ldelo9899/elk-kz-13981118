input {
  kafka {
    bootstrap_servers => "kafka01:9092"
    topics => ["winlogbeat","filebeat"]
    decorate_events => true
    codec => "json"
    ############################# HELK Kafka Group Consumption #############################
    # Enable logstash to not continously restart consumption of docs/logs it already has. However if you need it to, then change the 'group_id' value to something else (ex: could be a simple value like '100_helk_logstash')
    enable_auto_commit => "true"
    # During group_id or client_id changes, the kafka clinet will consume from earliest document so as not to lose data
    auto_offset_reset => "earliest"
    # If you have multiple logstash instances, this is your ID so that each instance consumes a slice of the Kafka pie.
    # No need to change this unless you know what your doing and for some reason have the need
    group_id => "helk_logstash"
    # Change to number of Kafka partitions, only change/set if scaling on large environment & customized your Kafka paritions
    # Default value is 1, read documentation for more info: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html#plugins-inputs-kafka-consumer_threads
    consumer_threads => 2
    ############################# HELK Optimizing Throughput #############################
    #fetch_min_bytes => "1024"
    #request_timeout_ms => "40000"
    ############################# HELK Optimizing Availability #############################
    #connections_max_idle_ms => "540000"
    #session_timeout_ms => "30000"
    #max_poll_interval_ms => "300000"
    #############################
    #max_poll_records => "500"
  }
}
output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "test"
  }
}
